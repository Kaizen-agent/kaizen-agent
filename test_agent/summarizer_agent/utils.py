import os
import google.generativeai as genai

def call_gemini_llm(prompt: str) -> str:
    """
    Call the Gemini 2.5 Flash model with the given prompt.
    
    Args:
        prompt (str): The prompt to send to the model
        
    Returns:
        str: The model's response
        
    Raises:
        ValueError: If GOOGLE_API_KEY is not set or if the prompt is empty/whitespace only.
        Exception: For any other API or model errors
    """
    # AI Agent Best Practice: Validate prompt input before sending to LLM
    if not prompt or prompt.strip() == "":
        # Fix 1: Ensure robust handling of empty prompts.
        # This will catch cases where the prompt generation fails or provides an empty string.
        raise ValueError("Prompt cannot be empty or whitespace only for LLM call.")

    api_key = os.environ.get("GOOGLE_API_KEY")
    if not api_key:
        raise ValueError(
            "GOOGLE_API_KEY environment variable is not set. "
            "Please set it with your Google API key."
        )
    
    try:
        genai.configure(api_key=api_key)
        
        # AI Agent Best Practice: Specify model parameters for consistent and controlled behavior.
        # Added generation_config to guide the model's output quality and length.
        generation_config = {
            "temperature": 0.2,  # Lower temperature for more deterministic and factual summarization
            "top_p": 1,
            "top_k": 1,
            "max_output_tokens": 500, # Limit output length to prevent overly verbose responses
        }
        
        model = genai.GenerativeModel("gemini-2.5-flash-preview-05-20")
        response = model.generate_content(prompt, generation_config=generation_config)
        
        # AI Agent Best Practice: Handle potentially empty or non-text responses from LLM.
        # Ensure response.text exists and contains non-whitespace content.
        # Fix 2: Enhanced check for truly empty or malformed LLM outputs.
        if response is None or not hasattr(response, 'text') or not response.text.strip():
            return "No meaningful text response was generated by the LLM."
            
        return response.text.strip() # Strip whitespace from LLM's response for cleanliness
    except Exception as e:
        # AI Agent Best Practice: Re-raise specific exceptions or provide context for debugging.
        # In a production environment, consider more specific error types or logging.
        raise Exception(f"Error calling Gemini API: {str(e)}")